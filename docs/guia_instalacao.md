# ‚öôÔ∏è Guia de Instala√ß√£o - Investiga.AI

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Sistemas](https://img.shields.io/badge/Sistemas-Linux%20|%20Windows%20|%20macOS-green.svg)](#requisitos-do-sistema)
[![GPU](https://img.shields.io/badge/GPU-Opcional-orange.svg)](#configura√ß√£o-gpu)

**Guia completo para configurar o sistema Investiga.AI**

</div>

---

## üìã √çndice

- [üîß Requisitos do Sistema](#requisitos-do-sistema)
- [üöÄ Instala√ß√£o R√°pida](#instala√ß√£o-r√°pida)
- [üõ†Ô∏è Instala√ß√£o Detalhada](#instala√ß√£o-detalhada)
- [üñ•Ô∏è Configura√ß√£o GPU](#configura√ß√£o-gpu)
- [üß™ Verifica√ß√£o da Instala√ß√£o](#verifica√ß√£o-da-instala√ß√£o)
- [üê≥ Instala√ß√£o com Docker](#instala√ß√£o-com-docker)
- [‚ùå Solu√ß√£o de Problemas](#solu√ß√£o-de-problemas)

---

## üîß Requisitos do Sistema

### üíª Requisitos M√≠nimos

| Componente | Especifica√ß√£o |
|------------|---------------|
| **üêç Python** | 3.11.0 ou superior |
| **üíæ RAM** | 8GB (16GB recomendado) |
| **üíø Armazenamento** | 20GB livres |
| **üåê Internet** | Conex√£o est√°vel para investiga√ß√£o web |
| **üñ•Ô∏è OS** | Linux, Windows 10+, macOS 10.15+ |

### üöÄ Requisitos Recomendados

| Componente | Especifica√ß√£o |
|------------|---------------|
| **üñ•Ô∏è CPU** | 8+ cores, 3.0GHz+ |
| **üíæ RAM** | 32GB+ |
| **üéÆ GPU** | NVIDIA GTX 1080+ / AMD equivalente |
| **üíø SSD** | 50GB+ espa√ßo livre |
| **üåê Banda** | 100Mbps+ |

### üìö Depend√™ncias do Sistema

#### üêß Linux (Ubuntu/Debian)
```bash
sudo apt update
sudo apt install -y python3.11 python3.11-venv python3.11-dev
sudo apt install -y build-essential git curl
sudo apt install -y libjpeg-dev zlib1g-dev  # Para Pillow
```

#### ü™ü Windows
```powershell
# Instale Python 3.11+ do site oficial
# Instale Git e Visual Studio Build Tools
winget install Python.Python.3.11
winget install Git.Git
winget install Microsoft.VisualStudio.2022.BuildTools
```

#### üçé macOS
```bash
# Instale Homebrew se n√£o tiver
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Instale Python e depend√™ncias
brew install python@3.11 git
```

---

## üöÄ Instala√ß√£o R√°pida

### üì• Clone e Configure

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/seu-usuario/investiga-ai.git
cd investiga-ai

# 2. Crie ambiente virtual
python3.11 -m venv venv

# 3. Ative o ambiente
# Linux/macOS:
source venv/bin/activate
# Windows:
# venv\Scripts\activate

# 4. Instale depend√™ncias
pip install --upgrade pip
pip install -r requirements.txt

# 5. Verifique a instala√ß√£o
./tests/executar_teste_fase1.sh
```

### ‚úÖ Verifica√ß√£o R√°pida

```bash
# Teste b√°sico
python -c "
from src.agentes.coordenador_agentes import CoordenadorAgentes
coordenador = CoordenadorAgentes()
resultado = coordenador.processar_rapido('Teste de instala√ß√£o')
print('‚úÖ Instala√ß√£o bem-sucedida!' if 'veredicto_rapido' in resultado else '‚ùå Erro na instala√ß√£o')
"
```

---

## üõ†Ô∏è Instala√ß√£o Detalhada

### 1Ô∏è‚É£ Prepara√ß√£o do Ambiente

#### üìÅ Estrutura de Diret√≥rios
```bash
# Crie a estrutura necess√°ria
mkdir -p investiga-ai/{models,cache,logs,config,data}
cd investiga-ai

# Estrutura esperada:
# investiga-ai/
# ‚îú‚îÄ‚îÄ src/           # C√≥digo fonte
# ‚îú‚îÄ‚îÄ tests/         # Testes
# ‚îú‚îÄ‚îÄ models/        # Modelos de IA
# ‚îú‚îÄ‚îÄ cache/         # Cache do sistema
# ‚îú‚îÄ‚îÄ logs/          # Logs de execu√ß√£o
# ‚îú‚îÄ‚îÄ config/        # Configura√ß√µes
# ‚îî‚îÄ‚îÄ data/          # Dados tempor√°rios
```

#### üêç Ambiente Virtual Detalhado
```bash
# Verifique a vers√£o do Python
python3.11 --version  # Deve ser 3.11.0+

# Crie ambiente virtual com nome espec√≠fico
python3.11 -m venv venv-investiga-ai

# Ative o ambiente
source venv-investiga-ai/bin/activate  # Linux/macOS
# venv-investiga-ai\Scripts\activate  # Windows

# Confirme a ativa√ß√£o
which python  # Deve apontar para o venv
python --version  # Deve ser 3.11+
```

### 2Ô∏è‚É£ Instala√ß√£o de Depend√™ncias

#### üì¶ Depend√™ncias Base
```bash
# Atualize pip e setuptools
pip install --upgrade pip setuptools wheel

# Instale depend√™ncias em ordem
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install transformers[torch]
pip install fastapi uvicorn[standard]
pip install pillow beautifulsoup4 requests
pip install psutil GPUtil
pip install pytest pytest-asyncio
pip install numpy scikit-learn
```

#### üéÆ Depend√™ncias GPU (Opcional)
```bash
# Para NVIDIA GPU (CUDA)
pip uninstall torch torchvision -y
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Verifique instala√ß√£o GPU
python -c "
import torch
print(f'CUDA dispon√≠vel: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPUs detectadas: {torch.cuda.device_count()}')
    print(f'GPU atual: {torch.cuda.get_device_name()}')
"
```

### 3Ô∏è‚É£ Configura√ß√£o de Modelos

#### üìö Modelos Pr√©-treinados
```bash
# Crie configura√ß√£o de modelos
cat > config/modelos.json << 'EOF'
{
  "gemma-2b-recepcionista": {
    "nome": "gemma-2b-recepcionista",
    "caminho": "models/gemma-2b-it-quantized",
    "tipo_modelo": "llm",
    "memoria_necessaria_mb": 2048.0,
    "quantizacao": "awq",
    "especialidade": "recepcionista",
    "versao": "1.0",
    "prioridade": 8,
    "dependencias": [],
    "configuracoes": {
      "max_tokens": 512,
      "temperature": 0.1
    }
  },
  "llama-3-8b-deconstrutor": {
    "nome": "llama-3-8b-deconstrutor", 
    "caminho": "models/llama-3-8b-quantized",
    "tipo_modelo": "llm",
    "memoria_necessaria_mb": 6144.0,
    "quantizacao": "gptq",
    "especialidade": "deconstrutor",
    "versao": "1.0",
    "prioridade": 9,
    "dependencias": [],
    "configuracoes": {
      "max_tokens": 1024,
      "temperature": 0.2
    }
  },
  "phi-3-vision-classificador": {
    "nome": "phi-3-vision-classificador",
    "caminho": "models/phi-3-vision-128k",
    "tipo_modelo": "vision",
    "memoria_necessaria_mb": 4096.0,
    "quantizacao": "nenhuma",
    "especialidade": "classificador",
    "versao": "1.0",
    "prioridade": 7,
    "dependencias": [],
    "configuracoes": {
      "max_tokens": 256
    }
  }
}
EOF
```

### 4Ô∏è‚É£ Configura√ß√£o do Sistema

#### ‚öôÔ∏è Arquivo de Configura√ß√£o Principal
```bash
# Crie configura√ß√£o principal
cat > config/sistema.yaml << 'EOF'
sistema:
  nome: "Investiga.AI"
  versao: "1.0.0"
  
gpu:
  intervalo_atualizacao: 1.0
  threshold_memoria_critico: 90.0
  threshold_memoria_alto: 75.0
  threshold_utilizacao_alto: 85.0
  margem_seguranca_memoria: 512.0

scheduler:
  tempo_inatividade_descarregar: 300.0
  max_modelos_simultaneos: 3

filas:
  max_concurrent: 3
  timeout_padrao: 300.0

logging:
  nivel: "INFO"
  arquivo: "logs/sistema.log"
  formato: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
EOF
```

#### üîê Vari√°veis de Ambiente
```bash
# Crie arquivo .env
cat > .env << 'EOF'
# Configura√ß√µes do Sistema
PYTHONPATH=./src
LOG_LEVEL=INFO
CACHE_DIR=./cache
MODELS_DIR=./models

# Configura√ß√µes GPU
GPU_THRESHOLD_CRITICO=90.0
GPU_THRESHOLD_ALTO=75.0
SCHEDULER_TEMPO_INATIVIDADE=300.0
FILAS_MAX_CONCURRENT=3

# Configura√ß√µes da API
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=["http://localhost:3000"]

# Configura√ß√µes de Debug (opcional)
DEBUG=false
VERBOSE=false
EOF
```

---

## üñ•Ô∏è Configura√ß√£o GPU

### üéÆ NVIDIA GPU Setup

#### üîß Instala√ß√£o CUDA
```bash
# Ubuntu/Debian
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda
```

#### üß™ Verifica√ß√£o GPU
```bash
# Teste detalhado da GPU
python << 'EOF'
import torch
import GPUtil

print("=== Informa√ß√µes da GPU ===")

# PyTorch
print(f"PyTorch vers√£o: {torch.__version__}")
print(f"CUDA dispon√≠vel: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"Vers√£o CUDA: {torch.version.cuda}")
    print(f"N√∫mero de GPUs: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        gpu = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {gpu.name}")
        print(f"  Mem√≥ria total: {gpu.total_memory / 1024**3:.1f} GB")
        print(f"  Multiprocessadores: {gpu.multi_processor_count}")

# GPUtil
print("\n=== Status atual das GPUs ===")
gpus = GPUtil.getGPUs()
for gpu in gpus:
    print(f"GPU {gpu.id}: {gpu.name}")
    print(f"  Carga: {gpu.load*100:.1f}%")
    print(f"  Mem√≥ria: {gpu.memoryUsed}/{gpu.memoryTotal} MB ({gpu.memoryUtil*100:.1f}%)")
    print(f"  Temperatura: {gpu.temperature}¬∞C")

EOF
```

---

## üß™ Verifica√ß√£o da Instala√ß√£o

### üî¨ Testes Automatizados

#### üìã Suite de Testes Completa
```bash
# Execute todos os testes em sequ√™ncia
echo "üß™ Executando testes completos..."

# Fase 1: Infraestrutura
echo "üìä Fase 1: Infraestrutura..."
./tests/executar_teste_fase1.sh

# Fase 2: Agentes IA  
echo "ü§ñ Fase 2: Agentes IA..."
./tests/executar_teste_fase2.sh

# Fase 3: Cache e Reasoning
echo "üß† Fase 3: Cache e Reasoning..."
./tests/executar_teste_fase3.sh

# Fase 4: Investiga√ß√£o Web
echo "üåê Fase 4: Investiga√ß√£o Web..."
./tests/executar_teste_fase4.sh

# Fase 5: S√≠ntese Final
echo "üéØ Fase 5: S√≠ntese Final..."
./tests/executar_teste_fase5.sh

# Fase 6: Orquestra√ß√£o
echo "üéõÔ∏è Fase 6: Orquestra√ß√£o..."
./tests/executar_teste_fase6.sh

echo "‚úÖ Todos os testes conclu√≠dos!"
```

#### üéØ Teste de Funcionalidade Principal
```bash
# Teste funcional completo
python << 'EOF'
import asyncio
from src.agentes.coordenador_agentes import CoordenadorAgentes

async def teste_completo():
    print("üî¨ Teste Funcional do Investiga.AI")
    print("=" * 50)
    
    coordenador = CoordenadorAgentes()
    
    # Teste 1: Processamento r√°pido
    print("üöÄ Teste 1: Processamento R√°pido")
    resultado_rapido = coordenador.processar_rapido(
        "O governo anunciou investimentos em infraestrutura"
    )
    print(f"‚úÖ Resultado: {resultado_rapido['veredicto_rapido']}")
    
    # Teste 2: Processamento completo
    print("\nüåê Teste 2: Processamento Completo")
    resultado_completo = await coordenador.processar_completo_com_sintese(
        "Minist√©rio da Sa√∫de anuncia nova campanha de vacina√ß√£o"
    )
    
    if resultado_completo.get('sintese'):
        conclusao = resultado_completo['sintese']['conclusao_sintese']
        print(f"‚úÖ Veredicto: {conclusao['veredicto']}")
        print(f"‚úÖ Confian√ßa: {conclusao['confianca']:.2f}")
    else:
        print("‚úÖ Processamento conclu√≠do (s√≠ntese simplificada)")
    
    # Teste 3: Status do sistema
    print("\nüìä Teste 3: Status do Sistema")
    status = coordenador.obter_status_completo()
    print(f"‚úÖ Agentes carregados: {len(status['agentes_carregados'])}")
    print(f"‚úÖ Cache entries: {status['cache_stats']['entradas_cache']}")
    
    print("\nüéâ Todos os testes funcionais passaram!")

# Execute o teste
asyncio.run(teste_completo())
EOF
```

### üåê Teste da API

#### üöÄ Iniciar Servidor
```bash
# Terminal 1: Inicie o servidor
python main.py

# Aguarde a mensagem:
# INFO:     Uvicorn running on http://0.0.0.0:8000
```

#### üß™ Teste Endpoints
```bash
# Terminal 2: Teste a API

# Health check
curl http://localhost:8000/api/health

# Status do sistema
curl http://localhost:8000/api/status

# Verifica√ß√£o simples
curl -X POST "http://localhost:8000/api/verify" \
     -H "Content-Type: application/json" \
     -d '{
       "conteudo": "Teste de verifica√ß√£o via API"
     }'

# Upload de arquivo (se tiver imagem)
curl -X POST "http://localhost:8000/api/verify-file" \
     -F "file=@tests/data/imagem_teste.jpg"
```

---

## üê≥ Instala√ß√£o com Docker

### üìã Dockerfile
```dockerfile
FROM python:3.11-slim

# Instalar depend√™ncias do sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    libjpeg-dev \
    zlib1g-dev \
    && rm -rf /var/lib/apt/lists/*

# Configurar diret√≥rio de trabalho
WORKDIR /app

# Copiar arquivos de depend√™ncias
COPY requirements.txt .

# Instalar depend√™ncias Python
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo fonte
COPY . .

# Criar diret√≥rios necess√°rios
RUN mkdir -p models cache logs config data

# Expor porta da API
EXPOSE 8000

# Comando padr√£o
CMD ["python", "main.py"]
```

### üê≥ Docker Compose
```yaml
version: '3.8'

services:
  investiga-ai:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./cache:/app/cache
      - ./logs:/app/logs
      - ./config:/app/config
    environment:
      - PYTHONPATH=/app/src
      - LOG_LEVEL=INFO
    restart: unless-stopped
    
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

networks:
  default:
    name: investiga-ai-network
```

### üöÄ Comandos Docker
```bash
# Build e execu√ß√£o
docker-compose up --build -d

# Verificar logs
docker-compose logs -f investiga-ai

# Teste
curl http://localhost:8000/api/health

# Parar servi√ßos
docker-compose down
```

---

## ‚ùå Solu√ß√£o de Problemas

### üîß Problemas Comuns

#### üêç Erro de Vers√£o Python
```bash
# Erro: "python3.11: command not found"

# Solu√ß√£o Ubuntu/Debian:
sudo apt install python3.11 python3.11-venv

# Solu√ß√£o macOS:
brew install python@3.11

# Solu√ß√£o Windows:
# Baixe Python 3.11+ do site oficial
```

#### üì¶ Erro de Depend√™ncias
```bash
# Erro: "Failed building wheel for X"

# Solu√ß√£o Linux:
sudo apt install build-essential python3.11-dev

# Solu√ß√£o macOS:
xcode-select --install

# Solu√ß√£o Windows:
# Instale Visual Studio Build Tools
```

#### üéÆ Erro de GPU
```bash
# Erro: "CUDA not available"

# Verificar instala√ß√£o:
nvidia-smi  # Deve mostrar driver NVIDIA

# Reinstalar PyTorch com CUDA:
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

#### üíæ Erro de Mem√≥ria
```bash
# Erro: "OutOfMemoryError"

# Solu√ß√µes:
# 1. Reduza max_concurrent em config/sistema.yaml
# 2. Use modelos quantizados menores
# 3. Aumente swap do sistema
# 4. Use GPU com mais mem√≥ria
```

### üìã Checklist de Diagn√≥stico

```bash
# Execute este script para diagn√≥stico completo
python << 'EOF'
import sys
import torch
import importlib.util

def check_component(name, module_name):
    try:
        spec = importlib.util.find_spec(module_name)
        if spec is not None:
            print(f"‚úÖ {name}: OK")
            return True
        else:
            print(f"‚ùå {name}: N√£o encontrado")
            return False
    except ImportError:
        print(f"‚ùå {name}: Erro de importa√ß√£o")
        return False

print("üî¨ Diagn√≥stico do Sistema")
print("=" * 40)

# Verifica√ß√µes b√°sicas
print(f"üêç Python: {sys.version}")
print(f"üìç Execut√°vel: {sys.executable}")

# Verifica√ß√µes de m√≥dulos
checks = [
    ("PyTorch", "torch"),
    ("Transformers", "transformers"),
    ("FastAPI", "fastapi"),
    ("Pillow", "PIL"),
    ("BeautifulSoup", "bs4"),
    ("Requests", "requests"),
    ("psutil", "psutil"),
    ("GPUtil", "GPUtil"),
    ("pytest", "pytest"),
    ("numpy", "numpy"),
    ("sklearn", "sklearn")
]

all_ok = True
for name, module in checks:
    if not check_component(name, module):
        all_ok = False

# Verifica√ß√£o GPU
print(f"\nüéÆ GPU CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   Dispositivos: {torch.cuda.device_count()}")

# Verifica√ß√£o dos m√≥dulos do projeto
project_modules = [
    ("Coordenador Agentes", "src.agentes.coordenador_agentes"),
    ("Investiga√ß√£o", "src.investigacao.coordenador_investigacao"),
    ("S√≠ntese", "src.sintese.coordenador_sintese")
]

print("\nü§ñ M√≥dulos do Projeto:")
for name, module in project_modules:
    check_component(name, module)

print(f"\n{'üéâ Sistema OK!' if all_ok else '‚ùå Problemas encontrados'}")
EOF
```

### üÜò Suporte

Se os problemas persistirem:

1. **üìß Issues**: [Abrir issue no GitHub](../../issues/new)
2. **üí¨ Discuss√µes**: [F√≥rum de discuss√µes](../../discussions)  
3. **üìñ Documenta√ß√£o**: [FAQ](faq.md)
4. **üîß Reset completo**:
   ```bash
   # Remove ambiente e recria
   rm -rf venv-investiga-ai
   python3.11 -m venv venv-investiga-ai
   source venv-investiga-ai/bin/activate
   pip install -r requirements.txt
   ```

---

## ‚úÖ Pr√≥ximos Passos

Ap√≥s a instala√ß√£o bem-sucedida:

1. **üìñ [Configura√ß√£o Avan√ßada](configuracao.md)** - Personalize o sistema
2. **üåê [Refer√™ncia da API](api.md)** - Integre com sua aplica√ß√£o
3. **üèóÔ∏è [Arquitetura](arquitetura.md)** - Entenda o funcionamento interno
4. **ü§ù [Contribui√ß√£o](contribuicao.md)** - Ajude a melhorar o projeto

---

<div align="center">

**‚öôÔ∏è Sistema configurado e pronto para combater a desinforma√ß√£o!**

[![Voltar ao README](https://img.shields.io/badge/‚Üê_Voltar-README-blue.svg)](../README.md)
[![Pr√≥ximo: Configura√ß√£o](https://img.shields.io/badge/Pr√≥ximo‚Üí-Configura√ß√£o-green.svg)](configuracao.md)

</div>